{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuSIETUezH9y"
      },
      "source": [
        "##### <font color=\"C13F38\"><strong>This homework is due at midnight on April 4, 2023. </strong></font>\n",
        "\n",
        "As always: run the cell below to get you started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9ijw-DnTzH92"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import uniform\n",
        "\n",
        "def reber_patterns():\n",
        "    # need to actual convert the sequences and then can probably rewrite this code to be more pythonic\n",
        "    reber_sequences = pd.read_csv('reber_grammar.csv')\n",
        "    pattern = {}\n",
        "    pattern['trainingInputs'] = {}\n",
        "    pattern['testingInputs'] = {}\n",
        "    rebertrain = reber_sequences['rebertrain'].values\n",
        "    rebertrain = rebertrain[:-1] # dropping the last NaN\n",
        "    rebertest = reber_sequences['rebertest'].values\n",
        "    characters = ['B','T','S','X','V','P','E']\n",
        "    nrCharacters = len(characters)\n",
        "    nrTrainingPatterns = len(rebertrain)\n",
        "    nrTestingPatterns = len(rebertest)\n",
        "\n",
        "    for i in range(nrTrainingPatterns):\n",
        "        sequenceLength = len(rebertrain[i])\n",
        "        pattern['trainingInputs'][i] = np.zeros((nrCharacters,sequenceLength))\n",
        "        for j in range(sequenceLength):\n",
        "            pattern['trainingInputs'][i][:,j] = np.array([rebertrain[i][j] == c for c in characters])\n",
        "    \n",
        "    for i in range(nrTestingPatterns):\n",
        "        sequenceLength = len(rebertest[i])\n",
        "        pattern['testingInputs'][i] = np.zeros((nrCharacters, sequenceLength))\n",
        "        for j in range(sequenceLength):\n",
        "            pattern['testingInputs'][i][:, j] = np.array([rebertest[i][j] == c for c in characters])\n",
        "    \n",
        "    pattern['trainingStrings'] = rebertrain[:-1]\n",
        "    pattern['testingStrings'] = rebertest\n",
        "    pattern['allStrings'] = np.concatenate((rebertrain, rebertest))\n",
        "    pattern['characters'] = characters\n",
        "\n",
        "    return pattern\n",
        "\n",
        "def logistic(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def activate(model, input, contextActs):\n",
        "    hiddenActs = logistic(model['i2hWeights']@input + model['c2hWeights']@contextActs)\n",
        "    outputActs = (np.exp(model['h2oWeights']@hiddenActs))/np.sum((np.exp(model['h2oWeights']@hiddenActs)))\n",
        "#     print(outputActs)\n",
        "    return outputActs, hiddenActs\n",
        "\n",
        "def evaluateModel(model, pattern):\n",
        "    nrTrainingInputs = len(pattern['trainingInputs'])-1\n",
        "    trainingAccuracy = np.zeros((nrTrainingInputs,1))\n",
        "\n",
        "    for p in range(nrTrainingInputs):\n",
        "        input = pattern['trainingInputs'][p]\n",
        "        sequenceLength = input.shape[1]\n",
        "        contextActs = 0.5 * np.ones((model['nrContexts'],1))\n",
        "        acc = np.zeros((sequenceLength-1,1))\n",
        "\n",
        "        for i in range(sequenceLength-1):\n",
        "            outputActs, hiddenActs = activate(model, input[:,i:(i+1)], contextActs)\n",
        "            contextActs = hiddenActs # recurrence\n",
        "            \n",
        "            history = pattern['trainingStrings'][p][:i+1]\n",
        "            prediction = history + pattern['characters'][np.argmax(outputActs)]\n",
        "            acc[i] = any([pattern['allStrings'][j].startswith(prediction)\n",
        "                           for j in range(len(pattern['allStrings']))\n",
        "                           if len(pattern['allStrings'][j]) >= len(prediction)]) # accuracy is wrong methinks\n",
        "            trainingAccuracy[p] = np.mean(acc)\n",
        "\n",
        "    # testing\n",
        "    nrTestingInputs = len(pattern['testingInputs'])-1\n",
        "    testingAccuracy = np.zeros((nrTestingInputs,1))\n",
        "    for p in range(nrTestingInputs):\n",
        "        input = pattern['testingInputs'][p]\n",
        "        sequenceLength = input.shape[1]\n",
        "        contextActs = 0.5 * np.ones((model['nrContexts'],1))\n",
        "        acc = np.zeros((sequenceLength-1,1))\n",
        "\n",
        "        for i in range(sequenceLength-1):\n",
        "            outputActs, hiddenActs = activate(model, input[:,i:(i+1)], contextActs)\n",
        "            contextActs = hiddenActs\n",
        "            prediction = np.argmax(outputActs)\n",
        "            history = pattern['testingStrings'][p][:i+1]\n",
        "            prediction = history + pattern['characters'][np.argmax(outputActs)]\n",
        "            acc[i] = any([pattern['allStrings'][j].startswith(prediction)\n",
        "                            for j in range(len(pattern['allStrings']))\n",
        "                            if len(pattern['allStrings'][j]) >= len(prediction)])\n",
        "            testingAccuracy[p] = np.mean(acc)\n",
        "    return trainingAccuracy, testingAccuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7I0o8HwzH94"
      },
      "source": [
        "# Lab 3.3 - Recurrent neural networks\n",
        "\n",
        "In this section of the lab, you will be exploring how a recurrent neural network is able to learn a rudimentary form of grammar.\n",
        "\n",
        "You will notice that you will spend less time coding up the models, and more time investigating its behavior. This is partly because you already spent a bunch of time coding up neural networks, and not so much time investigating how we can use them to investigate cognition. We hope we are giving you a better insight into how psychologists use neural networks to understand the mental processes they study.\n",
        "\n",
        "However, if you are truly interested in how the model in this lab work, we encourage you to dive into the code. You will see that the code is written _very much_ in the style of all the models you have seen, so hopefully that will help in making them a bit more interpretable.\n",
        "\n",
        "You can earn 25 points on this lab.\n",
        "\n",
        "_Credits_\n",
        "This first section of the assignment is a modification of the exercises on the Reber grammar given in the original PDP handbook by Jay McClelland, as well as the assignments based on this chapter by Todd Gureckis and Brenden Lake.\n",
        "\n",
        "This lab draws on work from the following publications:\n",
        "\n",
        "Cleeremans, A. and McClelland, J. L. (1991). Learning the structure of event sequences. Journal of Experimental Psychology General, 120, 235–253.\n",
        "\n",
        "Servan-Schreiber, D., Cleeremans, A., and McClelland, J. L. (1991). Graded state machines: The representation of temporal contingencies in simple recurrent networks. Machine Learning, 7, 161–193.\n",
        "\n",
        "# 1. Recurrent neural networks and the Reber grammar\n",
        "As we discussed in class, it is not trivial to design a neural network that is able to learn the temporal sequence of information. Of course, one could teach a simple feedforward network a list of input-output pairs in order, where the desired output of the previous pair is the input of the next one. However, there would be nothing inherently temporal about the learned associations. More importantly, the network would not be able to represent anything about the temporal context. In other words, it would not know what happened before the current input. In contrast, humans continuously rely on this information to make predictions.\n",
        "\n",
        "To deal with this limitation of feedforward neural networks, Jeffrey Elman designed the recurrent neural network in 1990:\n",
        "\n",
        "![rnn](rnn_3-3-1.png)\n",
        "\n",
        "A recurrent neural network looks a lot like a ‘regular’ neural network, except that its task is to use the current input to predict what the next input would be. In this way, it is trying to learn something about the _structure_ of the information that it faces. The network also differs from the ones we have studied before in its structure. As you surely noticed, the network has an extra set of ‘context’ nodes. These context nodes are essentially an extension of the input layer, but they have a special property: their activation is a _copy_ of the activity of the hidden layer on the previous time step.\n",
        "\n",
        "In essence, this induces a rudimentary form of memory into our model. It now not only knows what happens _right now_, but also what happened on the previous time step. This simple addition to the model has two implications. First, it makes backpropagation much more elaborate. We saw this in class, but luckily you will not have to worry about coding this up. Second, it allows the model to make predictions about what happens next based on the current information and the context in which this happened.\n",
        "\n",
        "To see why this is an important extension of neural networks, today we are going to expose a recurrent neural network to the “Reber grammar”. This is an artificial grammar that has a bunch of desirable properties that let us test whether the neural network will be able to use its temporal context to make informed predictions about what will happen next.\n",
        "\n",
        "# 1.1 The Reber grammar\n",
        "\n",
        "The rules of the artificial Reber grammar are best described in the flow chart below:\n",
        "\n",
        "![reber-grammar](reber-grammar.png)\n",
        "\n",
        "The grammar spells out a set of set of allowable or grammatical sequences of symbols. The flowchart above lets you construct these sequences! The grammar automatically starts with the left most arrow. If you are at a node, you follow an arrow from the current node to the next. Each arrow is associated with a letter, and when you transition through that arrow, you add the associated letter to the sequence. Each sequence you can get by going through this sequence is considered grammatical. Note that every sequence begins with B (for beginning) and ends with E (for end).\n",
        "\n",
        "So, for example, after our first transition, we can go down past P, then to the right past V, then up past another V, and then past E to the finish, so we have constructed the sequence ‘BPVVE’.\n",
        "\n",
        "There are many other sequences! For example, the self-connecting nodes will allow you to insert the infinite series of T and S into the sequences. Also, there is a loop in the flowchart that lets you insert infinite series of letters that do not immediately repeat. So, for example, ‘BTXSSSSSSSSSSSSSE’ and ‘BPTVPXVPXVPXVPXTVVE’ are both grammatical sequences. If a sequence cannot be constructed using this flowchart, it is considered not grammatical.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-anJecTDzH9-"
      },
      "source": [
        "<font color=\"508C46\"><strong>Question 1 (2 points)</strong><br>\n",
        "Let's make sure you understand the Reber grammar. Which of the following sequences are grammatical?\n",
        "1. BPTVVE\n",
        "2. BTSSSXXXVVE\n",
        "3. BPTTVPSE\n",
        "4. BTSSSXVPSE\n",
        "5. BTXXVVPXVVE\n",
        "<font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmk6ZZyvzH9_"
      },
      "source": [
        "\n",
        ".. double click this to type your response.\n",
        "\n",
        "1 and 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwk6jxotzH9_"
      },
      "source": [
        "Now, the neat thing about the Reber grammar is that in order to learn it, that is in order to be able to predict which character comes next, you cannot not pay attention to just the previous character. In fact, you need to incorporate the characters that occurred before that as well. To see this, you are going to pretend to be a ‘zero-order’, ‘first-order’, and ‘second-order’ memory predictor. In other words, you will have to make predictions about the next letter, based on different memory sizes. The zero-order memory predictor has no memory, not even knowing the current input. The first-order memory predictor can only remember one letter back. And, perhaps, unsurprisingly, the second-order memory predictor remembers two letters back."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f7PCFMVzH-S"
      },
      "source": [
        "<font color=\"508C46\"><strong>Question 2 (1 point)</strong><br>\n",
        "What is the zero-order structure of the grammar? In other words, if you did not even know the most recent letter, what can you predict about the output? You can find the answer to this question by figuring out the relative frequency of the various outputs. Of course, B is never a possible output (because each sequence starts automatically with that letter), but all other letters can be outputs. So, the zero-order structure is the relative frequency of each letter as an output.\n",
        "<font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81jCbTqGzH-S"
      },
      "source": [
        "\n",
        ".. double click this to type your response\n",
        "\n",
        "Frequency of every letter is 2 except for E which has a frequency of 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3Ackqk3zH-S"
      },
      "source": [
        "<font color=\"508C46\"><strong>Question 3 (2 point)</strong><br>\n",
        "**<u>What is the first-order structure of the grammar? In other words, for each the letters in this task (‘B’, ‘T’, ‘S’, ‘X’, ‘P’, ‘V’), what are the possible successors (following letters) given just a first-order memory?</u>** Hint: ‘B’ has two possible successors, and there are three letters with the same set of successors.\n",
        "<font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjme_eo-zH-T"
      },
      "source": [
        "\n",
        ".. double click this to type your response\n",
        "\n",
        "B has two successors T and P\n",
        "T has 4 succhessors, S, X, T, and V\n",
        "S has 3 successors, S, X, and E\n",
        "X has 4 successors, S, X, T, and V\n",
        "P has 4 successors, S, X, T and V\n",
        "V has 3 successors, V, E, P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osDQ1srqzH-T"
      },
      "source": [
        "<font color=\"508C46\"><strong>Question 4 (2 point)</strong><br>\n",
        "Finally, let’s explore the second-order structure of some of the grammar (we won’t go through the full possibilities because this will get too laborious). In the table below, we have given all possible two-order sequences that end in T or X (feel free to check our work). **<u>What are the possible successors?</u>**\n",
        "<font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDlEcbvNzH-V"
      },
      "source": [
        "Double click to fill out the following table:\n",
        "\n",
        "| Sequence | Successors |\n",
        "| ---------| ---------- |\n",
        "| BT |S, X |\n",
        "| TT |T,V |\n",
        "| XT |T,V | \n",
        "| PT | T,V| \n",
        "| TX | S,X| \n",
        "| SX |S,X | \n",
        "| XX | T,V| \n",
        "| PX | T,V| "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MslHKBDQzH-W"
      },
      "source": [
        "<font color=\"508C46\"><strong>Question 5 (4 points)</strong><br>\n",
        "**<u>Based on these questions, why is it useful to have a second-order memory in this task? Do you think it would be better to have third-order memory?</u>**\n",
        "<font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9u_pmRqzH-W"
      },
      "source": [
        "\n",
        ".. double click this to type your response.\n",
        "\n",
        "The higher order you go, the fewer output possibilities there are. This reduces error in a model. A third order memory, however, might risk not capturing the grammar fully"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG7BS8P2zH-W"
      },
      "source": [
        "# 2. Inspecting the recurrent neural network, and its input\n",
        "In 1991, Cleeremans and McClelland performed a study in which six participants learned the structure of a, slightly different, Reber grammar. Specifically, they performed 20 sessions of the task. In each of these, they responded to 3100 stimuli that followed this algorithm (The participants were instructed that the task was about motor learning, and so they pressed buttons according to six dots that appeared on a horizontal line on the screen. Each dot corresponded to one letter, but participants never saw the actual letters). So, people saw 62000 (!) trials of the Reber grammar. To test whether people were making predictions about the following letter, on 15% of the trials, a different, non-grammatical letter was substituted. Interestingly, Cleeremans and McClelland found that early in the experiment participants’ behavior largely followed the first-order structure that you reported in _Question 3_. They weren’t _surprised_ when a non-grammatical letter occurred, as long as that letter occurred in the first-order structure predicted by the previous letter. So, if the previous letter was ‘T’, their behavior suggested that people thought there were four possible next letters, even if there were only two (see Question 4)! Later on, however, participants became much more sensitive to the context in which the letters occurred, and so their behavior started following the second-order structure of the grammar much more.\n",
        "\n",
        "Here, we are going to use a recurrent neural network, to see if we can model this development in learning. The recurrent network looks like this:\n",
        "\n",
        "![rnn-3-3-2](rnn_3-3-2.png)\n",
        "\n",
        "\n",
        "It has 7 input neurons and 7 output neurons, one for each letter. On each ‘trial’ the network will be presented with a letter. Each of the input units will have activity 0, except for the current letter, which will be 1. This form coding is known as ‘one-hot’ encoding, because only one neuron is on or ‘hot’. The output that needs to be learned is simply the next letter, or the one-hot encoded version of that letter. To make this explicit, if the first letter in a sequence is B, and the next one is T (is this grammatical?), then the input is [1 0 0 0 0 0 0], and the correct output would be [0 1 0 0 0 0 0].\n",
        "\n",
        "\n",
        "In order to allow the network to encode the current ‘context’, on each step it receives additional input to the hidden layer from the context units (each have 8 units in our model). These units are special, because their activity is copied from the hidden units on the previous step (except for the first step, where they are all 0.5). This way, we are allowing the model to respond differently to the same letter when it occurs in different positions in the grammar.\n",
        "\n",
        "The context units represent what happened on the previous trial. So, they are a form of memory, allowing the network to connect consecutive inputs and (hopefully) learn the structure of the Reber grammar.\n",
        "\n",
        "Here are some technical specs about the types of activation functions. The neurons in the hidden layer use a logistic transformation over the net input, so that each neuron’s output is scaled between 0 and 1. The output layer, however, performs a _softmax_ activation function, so all activity across all neurons sums to 1. This is useful here, because we are trying to figure out which letter the network predicts will come next. This best takes form as a probability distribution, and the softmax gives us exactly that. You won’t see this in the lab, but using a softmax activation function actually makes backpropagation a bit easier!\n",
        "\n",
        "# 2.1 Inspecting the input and output patterns\n",
        "\n",
        "Let’s first inspect the patterns that we will use to train the neural network. You may notice that there is a .csv file in the data folder called reber_grammer.csv. If you open it, you will see that this file contains two columns, `rebertrain` and `rebertest`. If you inspect them, you will see that both these cell arrays contain sequences of the Reber grammar. Together, these represent all 43 possible sequences with maximum length 10, split as equally as possible between the two columns. There are two columns, because we will train the model on one set of sequences, and then test its performance on another set of sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dLVfrE-J_YHx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpChxMpIzH-X"
      },
      "source": [
        "<font color=\"508C46\"><strong>Question 6 (2 points)</strong><br>\n",
        "**<u>Why is it a good idea to train and test your model on different data?</u>**\n",
        "<font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27033MODzH-X"
      },
      "source": [
        "\n",
        ".. double click this to type your response.\n",
        "\n",
        "To make sure it is applicable to the cognitive process as a whole rather than overfitting to a specific dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVnyphBAzH-X"
      },
      "source": [
        "Of course, the sequences in these files are not in the correct format. The cell arrays contain strings, and not one-hot encoded vectors, so the recurrent neural network wouldn’t be able to handle it.\n",
        "This is where the function `reber_patterns` comes in handy. We wrote this for you. Let’s give it a try! Try the code below and look at the variable `pattern`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsvOf5eHzH-X",
        "outputId": "589cfe30-271b-4ca4-e6b7-b1fbad4954cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'trainingInputs': {0: array([[1., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 1., 0.],\n",
              "         [0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1.]]),\n",
              "  1: array([[1., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 0.],\n",
              "         [0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1.]]),\n",
              "  2: array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 1., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  3: array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  4: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  5: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 1., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  6: array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 1., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  7: array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  8: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  9: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  10: array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  11: array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  12: array([[1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0.],\n",
              "         [0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1.]]),\n",
              "  13: array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  14: array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 0., 0., 1., 1., 0.],\n",
              "         [0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  15: array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 0.],\n",
              "         [0., 1., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  16: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  17: array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 1., 0., 1., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  18: array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  19: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 1., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  20: array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.]])},\n",
              " 'testingInputs': {0: array([[1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 1., 0.],\n",
              "         [0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1.]]),\n",
              "  1: array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  2: array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 1., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  3: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  4: array([[1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0.],\n",
              "         [0., 1., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1.]]),\n",
              "  5: array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  6: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  7: array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  8: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  9: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  10: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 1., 1., 0.],\n",
              "         [0., 1., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  11: array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  12: array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 1., 1., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  13: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 1., 1., 1., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  14: array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  15: array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  16: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  17: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  18: array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  19: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  20: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
              "  21: array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]])},\n",
              " 'trainingStrings': array(['BTSXSE', 'BTXSE', 'BTSSSXSE', 'BTSSXXVVE', 'BTSSSXXVVE',\n",
              "        'BTXXVPXVVE', 'BTXXTTVVE', 'BTSXXTVVE', 'BTSSXXVPSE', 'BTSXXTVPSE',\n",
              "        'BTXXTVPSE', 'BTXXVPSE', 'BPVVE', 'BPTTVVE', 'BPTVPXVVE',\n",
              "        'BPVPXVVE', 'BPTVPXTVVE', 'BPVPXVPSE', 'BPTVPSE', 'BPVPXTVPSE'],\n",
              "       dtype=object),\n",
              " 'testingStrings': array(['BPTVVE', 'BPTTTVVE', 'BPTTTTVVE', 'BPTTTTTVVE', 'BPVPSE',\n",
              "        'BPTTVPSE', 'BPTTTTVPSE', 'BPVPXTVVE', 'BPVPXTTVVE', 'BPTVPXVPSE',\n",
              "        'BPTTVPXVVE', 'BTSSXSE', 'BTSSSSXSE', 'BTSSSSSXSE', 'BTXXVVE',\n",
              "        'BTXXTVVE', 'BTXXTTTVVE', 'BTXXTTVPSE', 'BTSXXVVE', 'BTSXXTTVVE',\n",
              "        'BTSSXXTVVE', 'BTSXXVPSE'], dtype=object),\n",
              " 'allStrings': array(['BTSXSE', 'BTXSE', 'BTSSSXSE', 'BTSSXXVVE', 'BTSSSXXVVE',\n",
              "        'BTXXVPXVVE', 'BTXXTTVVE', 'BTSXXTVVE', 'BTSSXXVPSE', 'BTSXXTVPSE',\n",
              "        'BTXXTVPSE', 'BTXXVPSE', 'BPVVE', 'BPTTVVE', 'BPTVPXVVE',\n",
              "        'BPVPXVVE', 'BPTVPXTVVE', 'BPVPXVPSE', 'BPTVPSE', 'BPVPXTVPSE',\n",
              "        'BPTTVPSE', 'BPTVVE', 'BPTTTVVE', 'BPTTTTVVE', 'BPTTTTTVVE',\n",
              "        'BPVPSE', 'BPTTVPSE', 'BPTTTTVPSE', 'BPVPXTVVE', 'BPVPXTTVVE',\n",
              "        'BPTVPXVPSE', 'BPTTVPXVVE', 'BTSSXSE', 'BTSSSSXSE', 'BTSSSSSXSE',\n",
              "        'BTXXVVE', 'BTXXTVVE', 'BTXXTTTVVE', 'BTXXTTVPSE', 'BTSXXVVE',\n",
              "        'BTSXXTTVVE', 'BTSSXXTVVE', 'BTSXXVPSE'], dtype=object),\n",
              " 'characters': ['B', 'T', 'S', 'X', 'V', 'P', 'E']}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "pattern = reber_patterns()\n",
        "pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPmiOjJMzH-e"
      },
      "source": [
        "The most important fields of this variable are `'trainingInputs'` and `'testingInputs'`. These are dictionaries of 21 and 22 keys respectively. Each value of these dictionaries again contains a sequence of letters, but now in a matrix and in one-hot encoded form. Let’s see what that looks like, by inspecting the content of the second training input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "djEBaMeWzH-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64706854-5ff0-4861-9e22-f27ca798a5c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 1., 0.],\n",
              "       [0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "pattern['trainingInputs'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TXybQkWzH-e"
      },
      "source": [
        "This input has 7 rows and 5 columns. If you inspect other trainingInputs, you will notice that all of them have 7 rows as well, but that the number of columns differs between them. So, the rows represent which letter occurs, and the columns indicated in which temporal sequence it occurs. Using the field characters of the pattern variable, we can now decode that this sequence corresponds to BTXSE. To see this, consider which rows have the one in them across columns, and then check how those indices correspond the letters in characters.\n",
        "\n",
        "Note that in the exercises for Lab 3.1, the patterns always had a desired output for each input. Why are there only inputs now?\n",
        "\n",
        "# 2.2 A bird's eye view of the model\n",
        "\n",
        "Take a look at `initModel`. As in Lab 3.1 exercises, this function will initialize the model. You’ll notice that the model 7 input neurons, 7 output neurons, 8 hidden neurons and 8 context neurons. There are connections between the hidden layer and the output layer, the input layer and the hidden layer, and the context layer and the hidden layer. All these connections will be modified over the course of learning. Note that initModel does not set the connection weights from the hidden layer to the context layer (why?).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0MpdtOrLzH-f"
      },
      "outputs": [],
      "source": [
        "def initModel():\n",
        "    model = {}\n",
        "    model['nrInputs'] = 7\n",
        "    model['nrHiddens'] = 8\n",
        "    model['nrContexts'] = model['nrHiddens']\n",
        "    model['nrOutputs'] = 7\n",
        "\n",
        "    model['h2oWeights'] = np.random.uniform(-0.35, 0.35, (model['nrOutputs'], model['nrHiddens']))\n",
        "    model['i2hWeights'] = np.random.uniform(-0.25, 0.25, (model['nrHiddens'], model['nrInputs']))\n",
        "    model['c2hWeights'] = np.random.uniform(-0.25, 0.25, (model['nrHiddens'], model['nrContexts']))\n",
        "        \n",
        "    model['learningRate'] = 0.01\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW3r29hBzH-f"
      },
      "source": [
        "# 2.3 Backpropagation through time\n",
        "We have established the structure of the network and what each input and corresponding output is going to be, so it seems like we are ready to train the model.\n",
        "\n",
        "How we are going to do this? The answer is very simple, and kind of complicated at the same time. The simple answer is that we are again going to use back propagation. So, after each sequence is presented, we are going to calculate the error of the network for each input, and then we are going to use this error to update the connection weights of the model (of course taking into account how active the projecting neurons were).\n",
        "\n",
        "This all sounds very much like what we have been doing, so where does it get complicated? Well, as you may remember from class, many steps of activity in a recurrent neural network can be viewed as single network that gets ‘folded’ out over time. Here is a diagram below that demonstrates this, with $x_t$, $h_t$, and $y_t$, the activity of the input, hidden, and output layer on trial t. And $U$, $V$, and $W$ the connection weights between the different layers.\n",
        "\n",
        "![rnn_3-3-3](rnn_3-3-3.png)\n",
        "\n",
        "Viewed this way, it’s easy to see that because the hidden layer is recurrent, the activity of the input neurons on any previous step will affect the output on the current step. The same can be said for the activity of the hidden neurons on any previous step. So, in order to update the connections from the hidden layer to the output layer based on the error on $t = 4$, we need to take _propagate this error back through time_, so that it can adjust the connection weights appropriately for each of the inputs. Now, we need to do this for step 3, as well, and so on.\n",
        "\n",
        "As you saw in class, this gets messy. And therefore, I will spare you the details. That being said, let’s have a quick look at `trainModel`, just so that you can develop an intuition for how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jBsNf80kzH-g"
      },
      "outputs": [],
      "source": [
        "# trainModel code\n",
        "def trainModel(nrEpochs):\n",
        "    model = initModel()\n",
        "    pattern = reber_patterns()\n",
        "    nrTrainingPatterns = len(pattern['trainingInputs'])\n",
        "\n",
        "    results = {}\n",
        "    results['trainingAccuracy'] = []\n",
        "    results['testingAccuracy'] = []\n",
        "    results['meanLL'] = np.zeros(nrEpochs)\n",
        "    results['model'] = []\n",
        "    \n",
        "    for epoch in range(nrEpochs):\n",
        "        # randomize the presentation\n",
        "        order = np.random.permutation(nrTrainingPatterns)\n",
        "        # initialize the loss function\n",
        "        loss = []\n",
        "        for i in range(nrTrainingPatterns):\n",
        "            \n",
        "            input = pattern['trainingInputs'][order[i]]\n",
        "            sequenceLength = input.shape[1]\n",
        "            \n",
        "            currentContextActs = 0.5 * np.ones((model['nrContexts'],1))\n",
        "            outputActs = np.zeros((model['nrOutputs'],sequenceLength-1))\n",
        "            hiddenActs = np.zeros((model['nrHiddens'],sequenceLength-1))\n",
        "            inputActs = np.zeros((model['nrInputs'],sequenceLength-1))\n",
        "            contextActs = hiddenActs.copy()\n",
        "            gradient = outputActs.copy()\n",
        "            \n",
        "            # here we activate the model, and predict the next letter\n",
        "            for j in range(sequenceLength-1): # don't predict last pattern\n",
        "                outputActs[:,j:(j+1)] , hiddenActs[:,j:(j+1)] = activate(model, input[:,j:(j+1)], currentContextActs)\n",
        "                contextActs[:,j:(j+1)] = currentContextActs\n",
        "                inputActs[:,j] = input[:,j]\n",
        "            \n",
        "                error_deriv = outputActs[:,j] - input[:,j+1]\n",
        "                gradient[:,j] = error_deriv\n",
        "                \n",
        "                # copy hidden activations to context\n",
        "                currentContextActs = hiddenActs[:,j:(j+1)]\n",
        "                \n",
        "                # calculate and store loss\n",
        "                loss_update = -np.sum(input[:,j+1] * np.log(outputActs[:,j]))\n",
        "                loss.append(loss_update)\n",
        "                        \n",
        "            delta_h2o = np.zeros((model['h2oWeights'].shape))\n",
        "            delta_i2h = np.zeros((model['i2hWeights'].shape))\n",
        "            delta_c2h = np.zeros((model['c2hWeights'].shape))\n",
        "            \n",
        "            # this is where backpropagation through time happens\n",
        "            for j in range(sequenceLength-1)[::-1]: # reversed\n",
        "                            \n",
        "                delta_h2o += gradient[:,j:(j+1)]*hiddenActs[:,j]\n",
        "                gradient_over_time = (model['h2oWeights'].T @ gradient[:,j:(j+1)])*hiddenActs[:,j:(j+1)] * (1 - hiddenActs[:,j:(j+1)])\n",
        "                \n",
        "                for k in range(j+1)[::-1]: # backprop through time\n",
        "                    \n",
        "                    delta_i2h += gradient_over_time*inputActs[:,k]\n",
        "                    delta_c2h += gradient_over_time*contextActs[:,k]\n",
        "\n",
        "                    gradient_over_time = (model['c2hWeights'].T @ gradient_over_time) * contextActs[:,k:(k+1)] * (1 - contextActs[:,k:(k+1)])\n",
        "                    \n",
        "            model['h2oWeights'] -= model['learningRate'] * delta_h2o\n",
        "            model['i2hWeights'] -= model['learningRate'] * delta_i2h\n",
        "            model['c2hWeights'] -= model['learningRate'] * delta_c2h\n",
        "                         \n",
        "        results['meanLL'][epoch] = np.nanmean(loss)\n",
        "        \n",
        "        if epoch % 20 == 0:\n",
        "            trainingAccuracy, testingAccuracy = evaluateModel(model, pattern)\n",
        "            results['trainingAccuracy'].append(np.mean(trainingAccuracy))\n",
        "            results['testingAccuracy'].append(np.mean(testingAccuracy))\n",
        "            \n",
        "    return model,results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VpIwBw8zH-g"
      },
      "source": [
        "A lot of this will look familiar, the script loads a model and the patterns, and then creates some vectors to store some evaluation results. Next, it trains the model for nrEpochs, which is an input parameter that we are going to mess with in a bit.\n",
        "\n",
        "On each epoch it goes through all training inputs (sequences) in random order. After picking a sequence, it first presents each letter, calculating the activation, the error, and the negative loglikelihood on each step.\n",
        "\n",
        "Then, it uses a double `for` loop to update the model, starting with the error on the very last time step, and back propagating that error to all previous time steps in the sequence. After this it will do the same for the second-to-last time step, continuing to backpropagate its error to all previous timesteps, and repeating the process until it has reached the first time step.\n",
        "\n",
        "Throughout this process, all the delta terms (changes in network weights) get added, and then they get applied, at the same time, to the connection weights.\n",
        "\n",
        "Finally, every 20 trials, `trainModel` runs a simple evaluation of how good the model is at predicting a valid successor for each time step of each sequence in the training and testing patterns (lines 76-82).\n",
        "\n",
        "If you want to understand the code, I’d be happy to walk you through it in more detail. However, this is not strictly necessary. As long as you have an intuition for how backpropagation through time works, and how it is different and the same as ‘regular’ backpropagation, you are ready to start training the model.\n",
        "\n",
        "## 2.3.1 Evaluation of model performance\n",
        "What is a good metric for evaluating model performance? As you know, simply looking at whether the model’s prediction follows the first-order structure is not enough. That way, we can a model could predict that BTS will be followed by E, and we would count it as correct.\n",
        "\n",
        "The function `evaluateModel` takes a better approach. It runs through a sequence and records the model’s prediction on each timestep (i.e., the neuron with the highest activation). For each step, it then counts this prediction as correct if it follows the grammar (Specifically, it runs through all the testing and training strings (because those are all possible sequences of length 10), and it figures out whether any of them start with the current history of letters followed by the predicted letter.).\n",
        "\n",
        "As we said above, this function gets called every 20 trials during `trainModel`, so we get a good insight into how our model’s performance is developing over time.\n",
        "\n",
        "# 3 Training and evaluating the model\n",
        "Let’s train the model! When using `trainModel`, you do not have to initialize the model or provide the training patterns, the function will do this automatically. The only thing you have to do is to provide a number of _epochs_. Let’s start with 500 epochs.\n",
        "\n",
        "In order to answer the next few questions, you need to train a model using `trainModel` for 500 epochs, and make sure to also store the secondary output `results`. Note that we are training the model on 500 × 21 = 10500 sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kduBrmK7zH-h"
      },
      "source": [
        "<font color=\"508C46\"><strong>Question 7 (2 points)</strong><br>\n",
        "Make a plot that depicts how the model’s accuracy on both the testing and the training sequences develops over time. **<u>Which accuracy seems to be higher? Why? Show the plot.</u>**\n",
        "<font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "InnawdshzH-h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "3909bdfe-ff3b-4e17-e4e2-9849b93d9585"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe47c1a7700>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# your code here\n",
        "model, results =trainModel(500)\n",
        "training_accuracy= results['trainingAccuracy']\n",
        "testing_accuracy= results['testingAccuracy']\n",
        "plt.plot(training_accuracy, label=\"training\")\n",
        "plt.plot(testing_accuracy, label=\"testing\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhLF0tYrzH-h"
      },
      "source": [
        "\n",
        ".. double click this to type your response.\n",
        "\n",
        "The training sequences has a higher level of accuracy than the testing sequence because that is what the model was traind on and there is thus some overfitting occuring. The testing still shows a pretty good learning/ accuracy though"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jrYHdVgzH-j"
      },
      "source": [
        "# 1.4 Visualizing the model's predictions\n",
        "Looking at performance scores gives us a good indication that the model is learning, but it doesn’t tell us how well it is doing. In order get a sense for this, we wrote a function called `showActivations`. This function takes as its input a model and a sequence of letters, and then it prints the predicted next letters (the activation in the output layer) for each step of the sequence.\n",
        "\n",
        "For example, here I run the function for a sequence with just the letter ‘B’:\n",
        "\n",
        "```python\n",
        "showActivations(model, 'B')\n",
        "```\n",
        "\n",
        "You can see that the model splits its prediction between T (56%) and P (42%). It is important to realize that the network does not always split its activity evenly between valid successors, because the training patterns are not precisely balanced in this way.\n",
        "\n",
        "Run the function `showActivations` using the trained `model`, and the sequence 'BTSXSE' as the input. Note that it makes reasonable predictions now, splitting its probability between two possible (valid) successors at each step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vPhebaW5zH-j"
      },
      "outputs": [],
      "source": [
        "def showActivations(model,sequence):\n",
        "    characters = ['B','T','S','X','V','P','E']\n",
        "    nrCharacters = len(characters)\n",
        "    sequenceLength = len(sequence)\n",
        "    input = np.zeros((nrCharacters,sequenceLength))\n",
        "    for i in range(sequenceLength):\n",
        "        input[:,i] = [sequence[i] == c for c in characters]\n",
        "    contextActs = 0.5 * np.ones((model['nrContexts'],1))\n",
        "\n",
        "    for j in range(sequenceLength):\n",
        "        if sequence[j] != 'E':\n",
        "            outputActs, hiddenActs = activate(model, input[:,j:(j+1)], contextActs)\n",
        "            contextActs = hiddenActs\n",
        "            print('Input: ', sequence[j], 'Output: ', characters[np.argmax(outputActs)])\n",
        "            for i in range(len(outputActs)):\n",
        "                print(characters[i], ': ', np.round(outputActs[i],2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "k_oTRZZzzH-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9005fbfb-be10-4bf7-c1b2-d70385773f83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  B Output:  T\n",
            "B :  [0.]\n",
            "T :  [0.57]\n",
            "S :  [0.]\n",
            "X :  [0.]\n",
            "V :  [0.01]\n",
            "P :  [0.41]\n",
            "E :  [0.]\n",
            "Input:  T Output:  S\n",
            "B :  [0.]\n",
            "T :  [0.]\n",
            "S :  [0.58]\n",
            "X :  [0.4]\n",
            "V :  [0.]\n",
            "P :  [0.]\n",
            "E :  [0.01]\n",
            "Input:  S Output:  X\n",
            "B :  [0.]\n",
            "T :  [0.]\n",
            "S :  [0.45]\n",
            "X :  [0.52]\n",
            "V :  [0.]\n",
            "P :  [0.]\n",
            "E :  [0.03]\n",
            "Input:  X Output:  X\n",
            "B :  [0.]\n",
            "T :  [0.]\n",
            "S :  [0.24]\n",
            "X :  [0.76]\n",
            "V :  [0.]\n",
            "P :  [0.]\n",
            "E :  [0.]\n",
            "Input:  S Output:  E\n",
            "B :  [0.]\n",
            "T :  [0.]\n",
            "S :  [0.01]\n",
            "X :  [0.01]\n",
            "V :  [0.01]\n",
            "P :  [0.01]\n",
            "E :  [0.96]\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "showActivations(model, ['B','T','S','X','S','E'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajyW-kX4zH-l"
      },
      "source": [
        "\n",
        "# 1.5 Characterizing the development of learning over time\n",
        "Now we return to our characterization of how learning this artificial grammar unfolds in humans. Recall that Cleeremans and McClelland had participants view this grammar for many trials. They were able to measure _how surprised_ people were by agrammatical substitutes. This in turn, let them figure out how much context people were using to make predictions. They found that early on during learning, people’s behavior suggested that they were making first-order predictions, whereas later on, as they mastered the grammar, their predictions incorporated a richer context (they took into account what happened a few trials back).\n",
        "\n",
        "Let’s see whether we can get something similar out of our networks!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myn19nlyzH-m"
      },
      "source": [
        "<font color=\"508C46\"><strong>Question 8 (10 points)</strong><br>\n",
        "Train three networks, one with only 5 epochs, one with 50 epochs, and another with 500 epochs of training. For each, use showEvaluations on at least two sequences from the <u>_test set_</u>. **<u>What has the network learned?</u>** You should motivate your answer with examples. Think about what kind of order in memory the model’s behavior is best described as. (How can you test what the memory order of the model is? Think about which testing sequences you want to use, and how they are able to inform you about what the model knows.)\n",
        "<font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pBDtfwXczH-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e071639-1550-4a3d-fbc9-4858e7a0b5e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  B Output:  V\n",
            "B :  [0.05]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.16]\n",
            "V :  [0.21]\n",
            "P :  [0.15]\n",
            "E :  [0.12]\n",
            "Input:  P Output:  V\n",
            "B :  [0.04]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.17]\n",
            "V :  [0.22]\n",
            "P :  [0.14]\n",
            "E :  [0.12]\n",
            "Input:  T Output:  V\n",
            "B :  [0.05]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.16]\n",
            "V :  [0.21]\n",
            "P :  [0.14]\n",
            "E :  [0.13]\n",
            "Input:  T Output:  V\n",
            "B :  [0.05]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.16]\n",
            "V :  [0.21]\n",
            "P :  [0.14]\n",
            "E :  [0.13]\n",
            "Input:  T Output:  V\n",
            "B :  [0.05]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.16]\n",
            "V :  [0.21]\n",
            "P :  [0.14]\n",
            "E :  [0.13]\n",
            "Input:  V Output:  V\n",
            "B :  [0.05]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.16]\n",
            "V :  [0.22]\n",
            "P :  [0.15]\n",
            "E :  [0.12]\n",
            "Input:  V Output:  V\n",
            "B :  [0.05]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.16]\n",
            "V :  [0.22]\n",
            "P :  [0.15]\n",
            "E :  [0.12]\n",
            "None\n",
            "Input:  B Output:  V\n",
            "B :  [0.05]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.16]\n",
            "V :  [0.21]\n",
            "P :  [0.15]\n",
            "E :  [0.12]\n",
            "Input:  P Output:  V\n",
            "B :  [0.04]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.17]\n",
            "V :  [0.22]\n",
            "P :  [0.14]\n",
            "E :  [0.12]\n",
            "Input:  V Output:  V\n",
            "B :  [0.05]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.16]\n",
            "V :  [0.22]\n",
            "P :  [0.15]\n",
            "E :  [0.12]\n",
            "Input:  P Output:  V\n",
            "B :  [0.04]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.16]\n",
            "V :  [0.22]\n",
            "P :  [0.14]\n",
            "E :  [0.12]\n",
            "Input:  X Output:  V\n",
            "B :  [0.04]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.16]\n",
            "V :  [0.21]\n",
            "P :  [0.15]\n",
            "E :  [0.12]\n",
            "Input:  T Output:  V\n",
            "B :  [0.05]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.16]\n",
            "V :  [0.21]\n",
            "P :  [0.14]\n",
            "E :  [0.13]\n",
            "Input:  V Output:  V\n",
            "B :  [0.05]\n",
            "T :  [0.16]\n",
            "S :  [0.15]\n",
            "X :  [0.16]\n",
            "V :  [0.22]\n",
            "P :  [0.15]\n",
            "E :  [0.12]\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "model, results =trainModel(5)\n",
        "print(showActivations(model,['B','P','T','T','T','V','V','E']))\n",
        "print(showActivations(model,['B','P','V','P','X','T','V','E']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjuu5E7azH-m"
      },
      "source": [
        "... double click this to type your response.\n",
        "\n",
        "Clearly, the model has not learned anything yet because the output is always V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GSlpEqCAzH-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6645b42-658a-4a79-8811-b08ef24100de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  B Output:  T\n",
            "B :  [0.]\n",
            "T :  [0.23]\n",
            "S :  [0.13]\n",
            "X :  [0.13]\n",
            "V :  [0.21]\n",
            "P :  [0.18]\n",
            "E :  [0.12]\n",
            "Input:  P Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.19]\n",
            "S :  [0.19]\n",
            "X :  [0.21]\n",
            "V :  [0.23]\n",
            "P :  [0.09]\n",
            "E :  [0.08]\n",
            "Input:  T Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.19]\n",
            "S :  [0.17]\n",
            "X :  [0.2]\n",
            "V :  [0.25]\n",
            "P :  [0.1]\n",
            "E :  [0.08]\n",
            "Input:  T Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.19]\n",
            "S :  [0.17]\n",
            "X :  [0.2]\n",
            "V :  [0.25]\n",
            "P :  [0.1]\n",
            "E :  [0.08]\n",
            "Input:  T Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.19]\n",
            "S :  [0.17]\n",
            "X :  [0.2]\n",
            "V :  [0.25]\n",
            "P :  [0.1]\n",
            "E :  [0.08]\n",
            "Input:  V Output:  P\n",
            "B :  [0.]\n",
            "T :  [0.13]\n",
            "S :  [0.1]\n",
            "X :  [0.11]\n",
            "V :  [0.22]\n",
            "P :  [0.25]\n",
            "E :  [0.19]\n",
            "Input:  V Output:  P\n",
            "B :  [0.01]\n",
            "T :  [0.12]\n",
            "S :  [0.11]\n",
            "X :  [0.12]\n",
            "V :  [0.2]\n",
            "P :  [0.24]\n",
            "E :  [0.21]\n",
            "None\n",
            "Input:  B Output:  T\n",
            "B :  [0.]\n",
            "T :  [0.23]\n",
            "S :  [0.13]\n",
            "X :  [0.13]\n",
            "V :  [0.21]\n",
            "P :  [0.18]\n",
            "E :  [0.12]\n",
            "Input:  P Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.19]\n",
            "S :  [0.19]\n",
            "X :  [0.21]\n",
            "V :  [0.23]\n",
            "P :  [0.09]\n",
            "E :  [0.08]\n",
            "Input:  V Output:  P\n",
            "B :  [0.]\n",
            "T :  [0.13]\n",
            "S :  [0.1]\n",
            "X :  [0.11]\n",
            "V :  [0.22]\n",
            "P :  [0.25]\n",
            "E :  [0.19]\n",
            "Input:  P Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.18]\n",
            "S :  [0.19]\n",
            "X :  [0.22]\n",
            "V :  [0.23]\n",
            "P :  [0.1]\n",
            "E :  [0.09]\n",
            "Input:  X Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.19]\n",
            "S :  [0.17]\n",
            "X :  [0.2]\n",
            "V :  [0.25]\n",
            "P :  [0.1]\n",
            "E :  [0.08]\n",
            "Input:  T Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.19]\n",
            "S :  [0.17]\n",
            "X :  [0.2]\n",
            "V :  [0.25]\n",
            "P :  [0.1]\n",
            "E :  [0.08]\n",
            "Input:  V Output:  P\n",
            "B :  [0.]\n",
            "T :  [0.13]\n",
            "S :  [0.1]\n",
            "X :  [0.11]\n",
            "V :  [0.22]\n",
            "P :  [0.25]\n",
            "E :  [0.19]\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "model, results =trainModel(50)\n",
        "print(showActivations(model,['B','P','T','T','T','V','V','E']))\n",
        "print(showActivations(model,['B','P','V','P','X','T','V','E']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW7ihArLzH-n"
      },
      "source": [
        "... you can also add text here\n",
        "\n",
        "The model is slightly better here. For instance, V outputs P rather than V, in accordance to the rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZbIh50qSzH-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b604ddd3-456a-420f-ad7f-2a3f15566391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  B Output:  T\n",
            "B :  [0.]\n",
            "T :  [0.57]\n",
            "S :  [0.]\n",
            "X :  [0.]\n",
            "V :  [0.01]\n",
            "P :  [0.42]\n",
            "E :  [0.]\n",
            "Input:  P Output:  T\n",
            "B :  [0.]\n",
            "T :  [0.59]\n",
            "S :  [0.01]\n",
            "X :  [0.]\n",
            "V :  [0.38]\n",
            "P :  [0.01]\n",
            "E :  [0.]\n",
            "Input:  T Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.33]\n",
            "S :  [0.02]\n",
            "X :  [0.01]\n",
            "V :  [0.64]\n",
            "P :  [0.01]\n",
            "E :  [0.]\n",
            "Input:  T Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.26]\n",
            "S :  [0.01]\n",
            "X :  [0.]\n",
            "V :  [0.73]\n",
            "P :  [0.]\n",
            "E :  [0.]\n",
            "Input:  T Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.2]\n",
            "S :  [0.]\n",
            "X :  [0.]\n",
            "V :  [0.8]\n",
            "P :  [0.]\n",
            "E :  [0.]\n",
            "Input:  V Output:  P\n",
            "B :  [0.]\n",
            "T :  [0.01]\n",
            "S :  [0.]\n",
            "X :  [0.]\n",
            "V :  [0.42]\n",
            "P :  [0.56]\n",
            "E :  [0.01]\n",
            "Input:  V Output:  E\n",
            "B :  [0.]\n",
            "T :  [0.]\n",
            "S :  [0.01]\n",
            "X :  [0.]\n",
            "V :  [0.]\n",
            "P :  [0.01]\n",
            "E :  [0.98]\n",
            "None\n",
            "Input:  B Output:  T\n",
            "B :  [0.]\n",
            "T :  [0.57]\n",
            "S :  [0.]\n",
            "X :  [0.]\n",
            "V :  [0.01]\n",
            "P :  [0.42]\n",
            "E :  [0.]\n",
            "Input:  P Output:  T\n",
            "B :  [0.]\n",
            "T :  [0.59]\n",
            "S :  [0.01]\n",
            "X :  [0.]\n",
            "V :  [0.38]\n",
            "P :  [0.01]\n",
            "E :  [0.]\n",
            "Input:  V Output:  P\n",
            "B :  [0.]\n",
            "T :  [0.01]\n",
            "S :  [0.]\n",
            "X :  [0.]\n",
            "V :  [0.24]\n",
            "P :  [0.7]\n",
            "E :  [0.04]\n",
            "Input:  P Output:  S\n",
            "B :  [0.]\n",
            "T :  [0.]\n",
            "S :  [0.51]\n",
            "X :  [0.49]\n",
            "V :  [0.]\n",
            "P :  [0.]\n",
            "E :  [0.]\n",
            "Input:  X Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.43]\n",
            "S :  [0.]\n",
            "X :  [0.]\n",
            "V :  [0.57]\n",
            "P :  [0.]\n",
            "E :  [0.]\n",
            "Input:  T Output:  V\n",
            "B :  [0.]\n",
            "T :  [0.13]\n",
            "S :  [0.]\n",
            "X :  [0.]\n",
            "V :  [0.86]\n",
            "P :  [0.]\n",
            "E :  [0.]\n",
            "Input:  V Output:  P\n",
            "B :  [0.]\n",
            "T :  [0.01]\n",
            "S :  [0.]\n",
            "X :  [0.]\n",
            "V :  [0.42]\n",
            "P :  [0.56]\n",
            "E :  [0.01]\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "model, results =trainModel(500)\n",
        "print(showActivations(model,['B','P','T','T','T','V','V','E']))\n",
        "print(showActivations(model,['B','P','V','P','X','T','V','E']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDHVlpwnzH-n"
      },
      "source": [
        "... and here!\n",
        "\n",
        "The model has learned it pretty well as shown in confidence levels"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "118b120627193aef794faea661e6204b26fa46f5be1a246b246eacb433382037"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}